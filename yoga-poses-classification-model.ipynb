{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/remonboshra/yoga-poses-classification-model?scriptVersionId=97737791\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# **Classification of Yoga posts**","metadata":{}},{"cell_type":"markdown","source":"Import Libraries","metadata":{}},{"cell_type":"code","source":"import os \nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pathlib import Path\nfrom matplotlib.image import imread\nimport plotly.express as px ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:29:00.303996Z","iopub.execute_input":"2022-05-07T10:29:00.30455Z","iopub.status.idle":"2022-05-07T10:29:07.297919Z","shell.execute_reply.started":"2022-05-07T10:29:00.304513Z","shell.execute_reply":"2022-05-07T10:29:07.297214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.list_physical_devices()  # APU device is visible to TensorFlow.","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:29:32.750495Z","iopub.execute_input":"2022-05-07T10:29:32.751187Z","iopub.status.idle":"2022-05-07T10:29:32.756302Z","shell.execute_reply.started":"2022-05-07T10:29:32.751152Z","shell.execute_reply":"2022-05-07T10:29:32.755492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '../input/yoga-pose-image-classification-dataset/dataset/'\nprint('Number of post to be predicted: ',len(os.listdir(base_dir)))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:29:39.991038Z","iopub.execute_input":"2022-05-07T10:29:39.991976Z","iopub.status.idle":"2022-05-07T10:29:40.025907Z","shell.execute_reply.started":"2022-05-07T10:29:39.991939Z","shell.execute_reply":"2022-05-07T10:29:40.025139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_dir = '../input/yoga-pose-image-classification-dataset/dataset/janu sirsasana'\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:29:44.802404Z","iopub.execute_input":"2022-05-07T10:29:44.802949Z","iopub.status.idle":"2022-05-07T10:29:44.806763Z","shell.execute_reply.started":"2022-05-07T10:29:44.802911Z","shell.execute_reply":"2022-05-07T10:29:44.805807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Viualize some of the Data","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=3,figsize=(10,5), subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(os.path.join(class_dir,os.listdir(class_dir)[i])))\n    ax.set_title(os.listdir(base_dir)[1])\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:29:57.768939Z","iopub.execute_input":"2022-05-07T10:29:57.769615Z","iopub.status.idle":"2022-05-07T10:29:58.491265Z","shell.execute_reply.started":"2022-05-07T10:29:57.769576Z","shell.execute_reply":"2022-05-07T10:29:58.490632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## count number of images in each class\nDF = pd.DataFrame(columns=['class','count'])\nDF['class']=pd.Series([os.listdir(base_dir)[x] for x in range(0,107)])\nDF['count']=pd.Series([len(os.listdir(os.path.join(base_dir,os.listdir(base_dir)[x]))) for x in range(0,107)])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:30:01.612763Z","iopub.execute_input":"2022-05-07T10:30:01.613025Z","iopub.status.idle":"2022-05-07T10:30:04.239633Z","shell.execute_reply.started":"2022-05-07T10:30:01.612995Z","shell.execute_reply":"2022-05-07T10:30:04.238879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(14,10))\ng=sns.barplot(x='class', y='count',data=DF)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:30:06.558598Z","iopub.execute_input":"2022-05-07T10:30:06.559433Z","iopub.status.idle":"2022-05-07T10:30:09.106529Z","shell.execute_reply.started":"2022-05-07T10:30:06.55939Z","shell.execute_reply":"2022-05-07T10:30:09.105904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model","metadata":{}},{"cell_type":"markdown","source":"Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import RandomRotation\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range = 0.1,\n                                  height_shift_range = 0.1,\n                                  shear_range = 0.1,\n                                  zoom_range= 0.1,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest',\n                                  validation_split=0.15)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:29:49.294521Z","iopub.execute_input":"2022-05-07T19:29:49.294781Z","iopub.status.idle":"2022-05-07T19:29:49.299998Z","shell.execute_reply.started":"2022-05-07T19:29:49.294752Z","shell.execute_reply":"2022-05-07T19:29:49.299306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_datagen.flow_from_directory(\n    base_dir,\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training')\n\nvalidation_data = train_datagen.flow_from_directory(\n    base_dir,\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:28:58.540612Z","iopub.execute_input":"2022-05-07T19:28:58.540868Z","iopub.status.idle":"2022-05-07T19:28:59.041433Z","shell.execute_reply.started":"2022-05-07T19:28:58.540838Z","shell.execute_reply":"2022-05-07T19:28:59.040615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import MaxPool2D, Flatten, Dense, Conv2D\n# covnet\nmodel = tf.keras.models.Sequential([Conv2D(128,(3,3),input_shape=(224,224,3),activation='relu'),\n                                   BatchNormalization(),\n                                   Conv2D(128,(3,3)),\n                                   BatchNormalization(),\n                                   MaxPool2D(2,2),\n                                   Conv2D(64,(3,3)),\n                                   BatchNormalization(),\n                                   Conv2D(64,(3,3)),\n                                   BatchNormalization(),\n                                   MaxPool2D(2,2),\n                                   Conv2D(32,(3,3)),\n                                   BatchNormalization(),\n                                   Conv2D(32,(3,3)),\n                                   BatchNormalization(),\n                                   MaxPool2D(2,2),\n                                   Flatten(),\n                                   Dense(1024,activation='relu'),\n                                   BatchNormalization(),\n                                   Dense(512,activation='relu'),\n                                   Dense(107,activation='softmax')])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:44:40.037839Z","iopub.execute_input":"2022-05-07T19:44:40.038122Z","iopub.status.idle":"2022-05-07T19:44:40.181476Z","shell.execute_reply.started":"2022-05-07T19:44:40.038092Z","shell.execute_reply":"2022-05-07T19:44:40.180785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compile\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(), metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:45:03.657863Z","iopub.execute_input":"2022-05-07T19:45:03.658147Z","iopub.status.idle":"2022-05-07T19:45:03.670438Z","shell.execute_reply.started":"2022-05-07T19:45:03.658115Z","shell.execute_reply":"2022-05-07T19:45:03.66964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index='0')):\n    history = model.fit(train_data, steps_per_epoch=120,validation_data=validation_data, epochs=30,verbose=1,\n                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.0001,patience=15,restore_best_weights=True)])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:31:47.305059Z","iopub.execute_input":"2022-05-07T20:31:47.305781Z","iopub.status.idle":"2022-05-07T21:12:21.375146Z","shell.execute_reply.started":"2022-05-07T20:31:47.305745Z","shell.execute_reply":"2022-05-07T21:12:21.374335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(validation_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:12:32.434836Z","iopub.execute_input":"2022-05-07T21:12:32.435114Z","iopub.status.idle":"2022-05-07T21:12:46.845502Z","shell.execute_reply.started":"2022-05-07T21:12:32.435064Z","shell.execute_reply":"2022-05-07T21:12:46.844763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Saving Model History\nwith open('./history0.pickle', 'wb') as file:\n  pickle.dump(history.history, file)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:13:34.803574Z","iopub.execute_input":"2022-05-07T21:13:34.80384Z","iopub.status.idle":"2022-05-07T21:13:34.809307Z","shell.execute_reply.started":"2022-05-07T21:13:34.803811Z","shell.execute_reply":"2022-05-07T21:13:34.808327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load history\nhistory_file = open('./history0.pickle', 'rb')\nloaded_history = pickle.load(history_file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Old CNN Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                  width_shift_range = 0.1,\n                                  height_shift_range = 0.1,\n                                  shear_range = 0.1,\n                                  zoom_range= 0.1,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest',\n                                  validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_datagen.flow_from_directory(\n    base_dir,\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training')\n\nvalidation_data = train_datagen.flow_from_directory(\n    base_dir,\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import MaxPool2D, Flatten, Dense, Conv2D\n# covnet\nmodel = tf.keras.models.Sequential([Conv2D(128,(3,3),input_shape=(224,224,3),activation='relu'),\n                                   Conv2D(128,(3,3)),\n                                   MaxPool2D(2,2),\n                                   Conv2D(64,(3,3)),\n                                   Conv2D(64,(3,3)),\n                                   MaxPool2D(2,2),\n                                   Conv2D(32,(3,3)),\n                                   Conv2D(32,(3,3)),\n                                   MaxPool2D(2,2),\n                                   Flatten(),\n                                   Dense(1024,activation='relu'),\n                                   Dense(512,activation='relu'),","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compile\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index='0')):\n    history = model.fit(train_data, steps_per_epoch=120,validation_data=validation_data, validation_steps=35, epochs=25,verbose=1,\n                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.01,patience=3,restore_best_weights=True)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(validation_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}